#!/usr/bin/env lua5.1

--------------------------------------------------------------------------------
-- luamarca: benchmarking utillity
-- This file is a part of luamarca library
-- Copyright (c) luamarca authors (see file `COPYRIGHT` for the license)
--------------------------------------------------------------------------------

require "lua-nucleo"

local create_common_stdout_logging
      = import 'lua-aplicado/common_logging.lua'
      {
        'create_common_stdout_logging'
      }

create_common_stdout_logging()

local load_tools_cli_data_schema,
      load_tools_cli_config,
      print_tools_cli_config_usage,
      freeform_table_value
      = import 'lua-aplicado/dsl/tools_cli_config.lua'
      {
        'load_tools_cli_data_schema',
        'load_tools_cli_config',
        'print_tools_cli_config_usage',
        'freeform_table_value'
      }

local is_table
      = import 'lua-nucleo/type.lua'
      {
        'is_table'
      }

local tkeys,
      tclone
      = import 'lua-nucleo/table-utils.lua'
      {
        'tkeys',
        'tclone'
      }

local tserialize
      = import 'lua-nucleo/tserialize.lua'
      {
        'tserialize'
      }

local ordered_pairs
      = import 'lua-nucleo/tdeepequals.lua'
      {
        'ordered_pairs'
      }

local fill_curly_placeholders,
      split_by_char
      = import 'lua-nucleo/string.lua'
      {
        'fill_curly_placeholders',
        'split_by_char'
      }

local shell_read
      = import 'lua-aplicado/shell.lua'
      {
        'shell_read'
      }

local format
      = import 'luamarca/formatter.lua'
      {
        'format'
      }

local load_benchmark
      = import 'luamarca/benchmark.lua'
      {
        'load_benchmark'
      }

--------------------------------------------------------------------------------

local TOOL_NAME = "luamarca"

local TOOL_HELP = [[

luamarca: benchmarks runner

Usage:

    luamarca [benchmark_file] [options]

Examples:

    luamarca bench/benchmark.lua
    luamarca bench/benchmark.lua --cases=foo
    luamarca bench/benchmark.lua --interpreters='luajit2 -jv:lua5.1'
    luamarca bench/benchmark.lua --iterations=1e7 --output=barchart

Options:

    --cases             Comma-separated list of cases to be benchmarked
    --info              Print list of available cases in the suite
                        without running benchmarks
    --interpreters      Colon-separated list of interpreters to be benchmarked
                        Default: 'lua:lua5.1:lua5.2:luajit2:luajit -O'
    --iterations        Number of iterations
                        Default: 1,000,000
    --output            Benchmark results format. Possible values:
                        'raw' - raw data
                        'table' - human-readable table
                        'barchart' - data file for gnuplot
                        Default: 'table
]]

--------------------------------------------------------------------------------

local CONFIG
do

  local CONFIG_SCHEMA = load_tools_cli_data_schema(function()
    cfg:root
    {
      cfg:node "luamarca"
      {
        cfg:existing_path "benchmark_file";
        -- TODO: fix config_dsl
        --       https://redmine-tmp.iphonestudio.ru/issues/3487
        --[[
        cfg:ilist "cases"
        {
          value = cfg:string "case";
        };
        ]]
        cfg:boolean "info";
        -- TODO: fix config_dsl
        --       https://redmine-tmp.iphonestudio.ru/issues/3487
        --[[
        cfg:ilist "foo" { value = cfg:string "value"; default = { ... } }
        cfg:non_empty_ilist "interpreters"
        {
          value = cfg:string "interpreter";
          default = { "lua", "lua5.1", "lua5.2", "luajit2", "luajit -O" };
        };
        ]]
        cfg:integer "iterations" { default = 1e6 };
        cfg:enum_value "output"
        {
          default = "table";
          values_set =
          {
            ["table"] = "table";
            ["raw"] = "raw";
            ["bargraph"] = "bargraph";
          };
        };
      };
    }
  end)

  local err
  CONFIG, err = load_tools_cli_config(
      function(args) -- Parse actions
        local param = { }

        param.benchmark_file = args[1]
        param.output = args["--output"]
        param.methods = args["--methods"]
        param.info = not not args["--info"]

        if args["--iterations"] then
          param.iterations = tonumber(args["--iterations"])
        end

        -- TODO: default should come from schema
        --       https://redmine-tmp.iphonestudio.ru/issues/3470
        param.interpreters = args["--interpreters"]
            and split_by_char(args["--interpreters"], ":")
            or { "lua", "lua5.1", "lua5.2", "luajit2", "luajit -O" }

        -- load benchmark to obtain list of cases
        -- TODO: allow benchmarks to specify interpreters they are meant for
        --       https://redmine-tmp.iphonestudio.ru/issues/3399#note-2
        --       E.g. FFI should not be benchmarked with plain Lua
        local benchmark, err = loadfile(param.benchmark_file)
        assert(benchmark, err)
        local ok, cases = pcall(benchmark)
        assert(
            ok and is_table(cases),
            "Benchmark must export table of case functions"
          )

        -- TODO: fixup config_dsl
        --       https://redmine-tmp.iphonestudio.ru/issues/3470
        param.cases = args["--cases"]
            and split_by_char(args["--cases"], ",")
            or tkeys(cases)

        return
        {
          PROJECT_PATH = args["--root"] or "./";
          [TOOL_NAME] = param;
        }
      end,
      TOOL_HELP,
      CONFIG_SCHEMA,
      nil, -- Specify primary config file with --base-config cli option
      nil, -- No secondary config file
      ...
    )

  if CONFIG == nil then
    print_tools_cli_config_usage(TOOL_HELP, CONFIG_SCHEMA)

    io.stderr:write("Error in tool configuration:\n", err, "\n\n")
    io.stderr:flush()

    os.exit(1)
  end

end

--------------------------------------------------------------------------------

local find_available_interpreters = function()
  local interpreters = { }

  local ints = freeform_table_value(CONFIG[TOOL_NAME].interpreters)

  for i = 1, #ints do
    local command = ints[i]
    local args = split_by_char(command, " ")

    if pcall(shell_read, "command", "-v", args[1]) then
      -- Lua 5.0 and 5.1 prints version to STDERR instead of STDOUT
      local res, name = pcall(shell_read, args[1], "-v", "2>&1")

      if res then
        -- Preserve original version if it cannot be parsed
        local ver = name:match("(%w+ [%w%p]+)")
        if ver then
          name = ver
        end

        -- NB: do not allow multiple use of the same interpreter
        -- FIXME: still, --interpreters="luajit:luajit -jv" should result in
        --        two distinct interpreters
        if not interpreters[ver] then
          interpreters[ver] = true
          interpreters[#interpreters + 1] =
          {
            name = name .. " (" .. command .. ")";
            command = args;
          }
        end
      end
    end
  end

  return interpreters
end

--
-- template runner script, ${var} to be replaced with config values later
--
local script = [[
require "lua-nucleo"
local tserialize = import "lua-nucleo/tserialize.lua" { "tserialize" }
local load_benchmark = import "luamarca/benchmark.lua" { "load_benchmark" }

local benchmark, err = load_benchmark("${benchmark_file}")
assert(benchmark, err)

benchmark.methods = ""

local res, err = benchmark:execute(${iterations})
if res then
  -- TODO: replace tserialize
  --       https://redmine-tmp.iphonestudio.ru/issues/3483
  io.write(tserialize(res), "\n")
else
  io.stderr:write(err, "\n")
end
]]

do
  -- Print benchmark methods and exit, if info cmdline option specified
  if CONFIG[TOOL_NAME].info then
    local benchmark, err = load_benchmark(CONFIG[TOOL_NAME].benchmark_file)

    if not benchmark then
      io.stderr:write(err, "\n")
      os.exit(0)
    end

    for method, _ in ordered_pairs(benchmark.handlers) do
      io.write("* ", method, "\n")
    end

    os.exit(0)
  end

  -- Compose runner script
  script = fill_curly_placeholders(script, CONFIG[TOOL_NAME])

  -- Execute benchmark for each interpreter in separate process
  local interpreters = find_available_interpreters()
  local results = { }
  for i = 1, #interpreters do
    local cmd = interpreters[i]

    local args = cmd.command
    args[#args + 1] = '-e'
    args[#args + 1] = script

    local ok, response = pcall(shell_read, unpack(args))

    local response_function = loadstring(response, "=" .. cmd.name)
    if response_function then
      results[cmd.name] = response_function()
    else
      io.stderr:write(
          "Cannot execute benchmark using ",
          cmd.name,
          ': ',
          response,
          "\n"
        )
    end
  end

  format(results, CONFIG[TOOL_NAME].output)
end
