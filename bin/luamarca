#!/usr/bin/env pk-lua-interpreter
-- TODO: should work with Lua 5.1 alone
--       https://github.com/logiceditor-com/luamarca/issues/1

--------------------------------------------------------------------------------
-- luamarca: benchmarking utillity
-- This file is a part of luamarca library
-- Copyright (c) luamarca authors (see file `COPYRIGHT` for the license)
--------------------------------------------------------------------------------

require "lua-nucleo"

local create_common_stdout_logging
      = import 'lua-aplicado/common_logging.lua'
      {
        'create_common_stdout_logging'
      }

create_common_stdout_logging()

local load_tools_cli_data_schema,
      load_tools_cli_config,
      print_tools_cli_config_usage,
      freeform_table_value
      = import 'lua-aplicado/dsl/tools_cli_config.lua'
      {
        'load_tools_cli_data_schema',
        'load_tools_cli_config',
        'print_tools_cli_config_usage',
        'freeform_table_value'
      }

local is_table
      = import 'lua-nucleo/type.lua'
      {
        'is_table'
      }

local tkeys,
      tclone
      = import 'lua-nucleo/table-utils.lua'
      {
        'tkeys',
        'tclone'
      }

local fill_curly_placeholders,
      split_by_char
      = import 'lua-nucleo/string.lua'
      {
        'fill_curly_placeholders',
        'split_by_char'
      }

local shell_read
      = import 'lua-aplicado/shell.lua'
      {
        'shell_read'
      }

local format
      = import 'luamarca/formatter.lua'
      {
        'format'
      }

--------------------------------------------------------------------------------

local TOOL_NAME = "luamarca"

local TOOL_HELP = [[

luamarca: benchmarks runner

Usage:

    luamarca [benchmark_file] [options]

Examples:

    luamarca bench/benchmark.lua
    luamarca bench/benchmark.lua --cases=foo
    luamarca bench/benchmark.lua --interpreters='luajit2 -jv:lua5.1'
    luamarca bench/benchmark.lua --iterations=1e7 --output=barchart

Options:

    --cases             Comma-separated list of cases to be benchmarked
    --info              Print list of available cases in the suite
                        without running benchmarks
    --interpreters      Colon-separated list of interpreters to be benchmarked
                        Default: 'lua:lua5.1:lua5.2:luajit2:luajit -O'
    --iterations        Number of iterations
                        Default: 1,000,000
    --output            Benchmark results format. Possible values:
                        'raw' - raw data
                        'table' - human-readable table
                        'barchart' - data file for gnuplot
                        Default: 'table
]]

--------------------------------------------------------------------------------

local CONFIG
do

  local CONFIG_SCHEMA = load_tools_cli_data_schema(function()
    cfg:root
    {
      cfg:node "luamarca"
      {
        cfg:existing_path "benchmark_file";
        -- TODO: fix config_dsl
        --       https://github.com/lua-aplicado/lua-aplicado/issues/37
        --[[
        cfg:ilist "cases"
        {
          value = cfg:string "case";
        };
        ]]
        cfg:boolean "info";
        -- TODO: fix config_dsl
        --       https://github.com/lua-aplicado/lua-aplicado/issues/37
        --[[
        cfg:ilist "foo" { value = cfg:string "value"; default = { ... } }
        cfg:non_empty_ilist "interpreters"
        {
          value = cfg:string "interpreter";
          default = { "lua", "lua5.1", "lua5.2", "luajit2", "luajit -O" };
        };
        ]]
        cfg:integer "iterations" { default = 1e6 };
        cfg:enum_value "output"
        {
          default = "table";
          values_set =
          {
            ["table"] = "table";
            ["raw"] = "raw";
            ["bargraph"] = "bargraph";
          };
        };
      };
    }
  end)

  local err
  CONFIG, err = load_tools_cli_config(
      function(args) -- Parse actions
        local param = { }

        param.benchmark_file = args[1]
        param.output = args["--output"]
        param.methods = args["--methods"]
        param.info = not not args["--info"]

        if args["--iterations"] then
          param.iterations = tonumber(args["--iterations"])
        end

        -- TODO: default should come from schema
        --       https://github.com/lua-aplicado/lua-aplicado/issues/38
        param.interpreters = args["--interpreters"]
            and split_by_char(args["--interpreters"], ":")
            or { "lua", "lua5.1", "lua5.2", "luajit2", "luajit -O" }

        -- load benchmark to obtain list of cases
        -- TODO: allow benchmarks to specify interpreters they are meant for
        --       https://github.com/logiceditor-com/luamarca/issues/3
        --       E.g. FFI should not be benchmarked with plain Lua
        local benchmark, err = loadfile(param.benchmark_file)
        assert(benchmark, err)
        local ok, cases = pcall(benchmark)
        assert(
            ok and is_table(cases),
            "Benchmark must export table of case functions"
          )

        -- TODO: fixup config_dsl
        --       https://github.com/lua-aplicado/lua-aplicado/issues/38
        param.cases = args["--cases"]
            and split_by_char(args["--cases"], ",")
            or tkeys(cases)

        return
        {
          PROJECT_PATH = args["--root"] or "./";
          [TOOL_NAME] = param;
        }
      end,
      TOOL_HELP,
      CONFIG_SCHEMA,
      nil, -- Specify primary config file with --base-config cli option
      nil, -- No secondary config file
      ...
    )

  if CONFIG == nil then
    print_tools_cli_config_usage(TOOL_HELP, CONFIG_SCHEMA)

    io.stderr:write("Error in tool configuration:\n", err, "\n\n")
    io.stderr:flush()

    os.exit(1)
  end

end

--------------------------------------------------------------------------------

local find_available_interpreters = function()
  local interpreters = { }

  local ints = freeform_table_value(CONFIG[TOOL_NAME].interpreters)

  for i = 1, #ints do
    local command = ints[i]
    local args = split_by_char(command, " ")

    if pcall(shell_read, "command", "-v", args[1]) then
      -- Lua 5.0 and 5.1 prints version to STDERR instead of STDOUT
      local res, name = pcall(shell_read, args[1], "-v", "2>&1")

      if res then
        -- Preserve original version if it cannot be parsed
        local ver = name:match("(%w+ [%w%p]+)")
        if ver then
          name = ver
        end

        -- NB: do not allow multiple use of the same interpreter
        -- TODO: still, --interpreters="luajit:luajit -jv" should result in
        --       two distinct interpreters
        --       https://github.com/logiceditor-com/luamarca/issues/2
        if not interpreters[ver] then
          interpreters[ver] = true
          interpreters[#interpreters + 1] =
          {
            name = name .. " (" .. command .. ")";
            command = args;
          }
        end
      end
    end
  end

  return interpreters
end

--
-- template runner script, ${var} to be replaced with config values later
--
local script = [[
require "socket"
local gettime = socket.gettime

-- load benchmark file
local benchmark, err = loadfile("${benchmark_file}")
assert(benchmark, err)

-- obtain list of cases
local ok, cases = pcall(benchmark)
assert(ok)

local handler = assert(
    cases["${case}"],
    "Benchmark does not export '${case}' function via return table"
  )

-- measure
local time = gettime()
for i = 1, ${iterations} do
  handler()
end
time = gettime() - time

-- report elapsed time
io.write(time, "\n")
]]

do
  -- config
  local config =
  {
    cases = freeform_table_value(CONFIG[TOOL_NAME].cases);
    benchmark_file = CONFIG[TOOL_NAME].benchmark_file;
    iterations = CONFIG[TOOL_NAME].iterations;
  }

  -- print benchmark cases and exit, if info cmdline option specified
  -- NB: cases taken from config, list may be overriden with --cases=LIST option
  if CONFIG[TOOL_NAME].info then
    table.sort(config.cases)
    for i = 1, #config.cases do
      io.write("* ", config.cases[i], "\n")
    end
    os.exit(0)
  end

  -- timing results for interpreters
  local results = { }

  -- execute each case of benchmark in each interpreter in separate processes
  local interpreters = find_available_interpreters()
  for i = 1, #interpreters do
    local cmd = interpreters[i]

    local args = tclone(cmd.command)
    args[#args + 1] = '-e'
    -- NB: will be replaced in inner loop
    args[#args + 1] = script

    -- timing results for cases
    local case_results = { }
    results[cmd.name] = case_results

    -- for each case
    for j = 1, #config.cases do
      config.case = config.cases[j]
      -- override script
      args[#args] = fill_curly_placeholders(script, config)
      -- run script
      local ok, response = pcall(shell_read, unpack(args))
      -- if run ok
      if ok then
        -- collect timing
        -- NB: format to conform formatter.lua
        case_results[#case_results + 1] =
        {
          iterations = config.iterations;
          name = config.case;
          timing =  tonumber(response);
        }
      -- failed to run the case
      else
        -- report error
        io.stderr:write(
            "Cannot execute benchmark using ",
            cmd.name,
            ': ',
            response,
            "\n"
          )
      end

    end -- cases

  end -- interpreters

  -- finally print results
  format(results, CONFIG[TOOL_NAME].output)
end
